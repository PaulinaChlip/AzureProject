name: CI/CD Pipeline

on:
  push:
    branches: [ main ]         # uruchamia się przy pushu na main
  workflow_dispatch:           

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    env:
      EVENTHUB_CONNECTION_STRING: ${{ secrets.EVENTHUB_CONNECTION_STRING }}
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
    #repo checkout
    - name: Checkout repository
      uses: actions/checkout@v3

    #terraform
    #- name: Set up Terraform
    #  uses: hashicorp/setup-terraform@v2
    #  with:
     #   terraform_version: 1.5.0

    #- name: Terraform Init
    #  working-directory: ./iac
    #  run: terraform init

    #- name: Terraform Plan
    #  working-directory: ./iac
    #  run: terraform plan

    # - name: Terraform Apply
    #   working-directory: ./iac
    #   run: terraform apply -auto-approve

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Send events to Event Hub
      run: python scripts/send_events.py

    - name: Run Databricks notebook
      run: |
        # Import notebook do Databricks workspace (opcjonalne)
        databricks workspace import --overwrite notebooks/aggregate_data.py /Users/<you>/aggregate_data.py
        
        # Wywołanie joba (JSON definiuje notebook, cluster, parametry)
        databricks runs submit --json-file runs_submit.json

